{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcBJMxI47A7ksDtqgLbB3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debojit11/ml_nlp_dl_transformers/blob/main/DL_week_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12: MLPs – Neural Networks for NLP"
      ],
      "metadata": {
        "id": "IPQzbMQv8DAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **SECTION 1: Welcome & Objectives**"
      ],
      "metadata": {
        "id": "6RnoPFBV8E6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvSW7oJL78bB",
        "outputId": "edeea976-f901-498a-fc87-64eefa5f8a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Week 12!\n",
            "This week, you'll:\n",
            "- Understand how Multi-Layer Perceptrons (MLPs) work\n",
            "- Build a simple MLP from scratch using PyTorch\n",
            "- Use it for text classification (spam vs ham)\n",
            "- Compare it to classical models like Logistic Regression\n"
          ]
        }
      ],
      "source": [
        "print(\"Welcome to Week 12!\")\n",
        "print(\"This week, you'll:\")\n",
        "print(\"- Understand how Multi-Layer Perceptrons (MLPs) work\")\n",
        "print(\"- Build a simple MLP from scratch using PyTorch\")\n",
        "print(\"- Use it for text classification (spam vs ham)\")\n",
        "print(\"- Compare it to classical models like Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 2: What’s an MLP?**"
      ],
      "metadata": {
        "id": "wzzvw1WG8N84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What’s an MLP (Multi-Layer Perceptron)?\n",
        "An MLP is a type of **feedforward neural network**.  \n",
        "It has layers of **neurons** with weights and activations.\n",
        "\n",
        "Structure:\n",
        "- Input Layer (e.g., TF-IDF vector)\n",
        "- One or more **Hidden Layers** with ReLU\n",
        "- Output Layer (e.g., probability of spam)\n",
        "\n",
        "MLPs learn patterns in data using **backpropagation** and **gradient descent**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SvAln1Hr8R7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✍️ Example: Spam Classifier with MLP (PyTorch)\n",
        "\n",
        "- Input: 1000-d TF-IDF vector from SMS messages\n",
        "- Output: Probability that message is spam\n",
        "- Architecture:\n",
        "  - Linear(1000 → 128) + ReLU\n",
        "  - Dropout\n",
        "  - Linear(128 → 1) + Sigmoid\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "O9OpdrJb88Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 3: Load & Vectorize SMS Data**"
      ],
      "metadata": {
        "id": "EDwH1Ikr9AZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✍️ Example: Spam Classifier with MLP (PyTorch)\n",
        "\n",
        "We're building a spam classifier using PyTorch. Here's how the key components work:\n",
        "\n",
        "- **Input:** Each SMS message is transformed into a 1000-dimensional **TF-IDF vector**, representing the importance of words in the message."
      ],
      "metadata": {
        "id": "atHqsUGn-dOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "QdSP1Da19ACt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', names=[\"label\", \"message\"])\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "gfOu9aJI9LIv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
        "X = vectorizer.fit_transform(df['message']).toarray()\n",
        "y = df['label_num'].values"
      ],
      "metadata": {
        "id": "1m9KiyZU9NwQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fBxAJeHa9Qrv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 4: PyTorch Dataset & Dataloader**"
      ],
      "metadata": {
        "id": "WPpS7s2U9Stf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dataset and DataLoader (Section 4):**\n",
        "    - We create a custom `SMSDataset` class to efficiently load and access our SMS data (features and labels).\n",
        "    - The `DataLoader` then wraps this dataset, providing batches of data for training and testing, and handles shuffling the training data to prevent learning order-dependent patterns."
      ],
      "metadata": {
        "id": "A3Uhzi5r-fAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SMSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "xpVI3-j3-iff"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = SMSDataset(X_train, y_train)\n",
        "test_ds = SMSDataset(X_test, y_test)\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "fz9mUfnu-rdx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 5: Define MLP Model**"
      ],
      "metadata": {
        "id": "ID-LuQrS-www"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MLP Model (Section 5):**\n",
        "    - We define our `MLP` class, inheriting from `nn.Module`.\n",
        "    - The network architecture consists of:\n",
        "        - A **Linear layer** that takes the 1000-dimensional input and transforms it into a 128-dimensional hidden representation.\n",
        "        - A **ReLU (Rectified Linear Unit) activation function** introduces non-linearity, allowing the network to learn complex relationships.\n",
        "        - **Dropout** is a regularization technique that randomly sets a fraction (e.g., 0.2) of the neurons to zero during training, preventing overfitting.\n",
        "        - Another **Linear layer** maps the 128-dimensional hidden representation to a 1-dimensional output.\n",
        "        - A **Sigmoid activation function** squashes the output to a value between 0 and 1, representing the probability of the message being spam."
      ],
      "metadata": {
        "id": "cK51JhcP-yQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "JFLIQ6Nj-8Iy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "JsoPqIvi-9LC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(X.shape[1])"
      ],
      "metadata": {
        "id": "qvNFWE0R-_fD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 6: Train the MLP**"
      ],
      "metadata": {
        "id": "eG_dmFXm_6zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Training Loop (Section 6):**\n",
        "    - We define the **loss function** (`nn.BCELoss`) suitable for binary classification.\n",
        "    - We choose an **optimizer** (`Adam`) to update the model's weights based on the gradients.\n",
        "    - The training loop iterates through the data for a specified number of **epochs**:\n",
        "        - In each epoch, the model is set to **training mode** (`model.train()`).\n",
        "        - We iterate through the batches of data provided by the `train_dl`.\n",
        "        - For each batch, we perform the **forward pass** to get predictions.\n",
        "        - We calculate the **loss** by comparing the predictions to the true labels.\n",
        "        - We perform **backpropagation** (`loss.backward()`) to compute gradients.\n",
        "        - We update the model's weights using the **optimizer** (`optimizer.step()`).\n",
        "        - We reset the gradients before the next batch (`optimizer.zero_grad()`).\n",
        "        - After each epoch, the model is set to **evaluation mode** (`model.eval()`).\n",
        "        - We disable gradient calculation (`with torch.no_grad()`) during evaluation to save memory and computation.\n",
        "        - We iterate through the `test_dl` to get predictions on the test set.\n",
        "        - We calculate and print the **accuracy** of the model on the test set."
      ],
      "metadata": {
        "id": "cBQcL8sT_8Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "ob9koztj_Fak"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "bmcCotlLAC5D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        preds = model(xb).squeeze()\n",
        "        loss = loss_fn(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Eval\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_preds = []\n",
        "        for xb, yb in test_dl:\n",
        "            preds = model(xb).squeeze()\n",
        "            all_preds.extend((preds > 0.5).int().tolist())\n",
        "        acc = accuracy_score(y_test, all_preds)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq1JpLodAEdY",
        "outputId": "e415b150-ecc5-4b24-c5ba-4839d3baac46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Accuracy: 0.9892\n",
            "Epoch 2, Accuracy: 0.9892\n",
            "Epoch 3, Accuracy: 0.9892\n",
            "Epoch 4, Accuracy: 0.9892\n",
            "Epoch 5, Accuracy: 0.9892\n",
            "Epoch 6, Accuracy: 0.9892\n",
            "Epoch 7, Accuracy: 0.9892\n",
            "Epoch 8, Accuracy: 0.9892\n",
            "Epoch 9, Accuracy: 0.9892\n",
            "Epoch 10, Accuracy: 0.9892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Output:** Probability that message is spam (a value between 0 and 1).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N03nDdiqATMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Why Use MLPs for Text?\n",
        "\n",
        "Pros:\n",
        "- Can learn **non-linear patterns** in the data, potentially capturing more complex relationships than linear models.\n",
        "- More **expressive** than simple models like logistic regression, especially with multiple hidden layers.\n",
        "- Can leverage different input representations like **embeddings** (dense vector representations of words) or simpler methods like TF-IDF.\n",
        "\n",
        "Cons:\n",
        "- **Don’t inherently capture word order** or sequential dependencies in the text, as they treat the input as a bag of features.\n",
        "- May **struggle with long sequences** as the input size can become very large.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fzQ_T31cAXzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Performance Notes\n",
        "\n",
        "On simple bag-of-words features like TF-IDF:\n",
        "- MLP performance is often **comparable to or slightly better** than logistic regression for tasks like spam classification.\n",
        "- Using **deeper networks** (more hidden layers) might improve performance but also increases the risk of **overfitting**, especially with limited data. Careful tuning and regularization are crucial.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Real-World Use Cases\n",
        "\n",
        "| Use Case                  | MLP Role                             |\n",
        "|---------------------------|--------------------------------------|\n",
        "| Spam/Ham Classifier       | Text classification using TF-IDF features. |\n",
        "| Sentiment Analysis        | Provides a simple deep learning baseline, especially with aggregated word embeddings as input. |\n",
        "| Intent Detection          | Can be effective when used with pre-trained word embeddings or sentence embeddings as input features. |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "M_2Y41C_AcvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚧 Limitations\n",
        "\n",
        "- MLPs treat input features independently and **don’t inherently understand word order** or sequential context.\n",
        "- Unlike Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), MLPs **don’t share weights** across different parts of the input, making them less efficient for processing sequential data.\n",
        "- Performance can be **highly sensitive to the input representation**. The quality of the features (e.g., TF-IDF, embeddings) significantly impacts the model's ability to learn.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GsRCc2LtAgJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Next week, we’ll go from MLPs to **Recurrent Neural Networks (RNNs)**, which are built for **sequential data** like sentences! 🧠📈"
      ],
      "metadata": {
        "id": "PHO9ImFgAhmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 7: Wrap-up**"
      ],
      "metadata": {
        "id": "LktJazf-CMgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎉 You Trained an MLP!\n",
        "- It learned to classify messages as spam/ham\n",
        "- You used a **dense neural network** with hidden layers\n",
        "- Performance is often similar to logistic regression with TF-IDF, but can improve with more features, deeper models, or embeddings\n",
        "\n",
        "➡️ Next week: we explore **RNNs & LSTMs** for handling sequential data like sentences."
      ],
      "metadata": {
        "id": "sDnpjM5rCNwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 8: Exercises**"
      ],
      "metadata": {
        "id": "JCHhKFsDCQHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍️ Exercises:\n",
        "1. Change the number of hidden units and observe accuracy.\n",
        "2. Add another hidden layer and compare.\n",
        "3. Use other vectorizers like `CountVectorizer` or `HashingVectorizer`.\n",
        "4. Try embeddings instead of TF-IDF for input."
      ],
      "metadata": {
        "id": "sG7TcPnRCTJn"
      }
    }
  ]
}