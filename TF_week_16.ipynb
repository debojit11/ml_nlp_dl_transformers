{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBRnqTvWRcDj90h2txhtFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debojit11/ml_nlp_dl_transformers/blob/main/TF_week_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Week 16 ‚Äì Modern NLP (T5, BART, GPT)\n",
        "\n",
        "---\n",
        "\n",
        "## üåü What is ‚ÄúModern NLP‚Äù?\n",
        "\n",
        "Modern NLP models go beyond understanding ‚Äî they can:\n",
        "- **Generate** text\n",
        "- **Rewrite** or **summarize** content\n",
        "- **Translate** between languages\n",
        "- Handle **multi-task** setups with a single architecture\n",
        "\n",
        "They use **encoder-decoder** or **decoder-only** Transformer variants.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Overview of T5, BART, GPT\n",
        "\n",
        "| Model | Type               | Pretraining Objective           | Use Cases                         |\n",
        "|-------|--------------------|----------------------------------|-----------------------------------|\n",
        "| T5    | Encoder-Decoder    | Text-to-text tasks              | QA, summarization, translation    |\n",
        "| BART  | Encoder-Decoder    | Corrupted text reconstruction   | Summarization, generation         |\n",
        "| GPT   | Decoder-Only       | Next-token prediction           | Text generation, chatbots         |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ BART: Denoising Autoencoder\n",
        "\n",
        "BART is trained by **corrupting input text** (e.g., deleting, shuffling, masking)  \n",
        "and **learning to reconstruct** the original.\n",
        "\n",
        "Useful for:\n",
        "- Summarization\n",
        "- Rephrasing/paraphrasing\n",
        "- Style transfer\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "text = '''BART is a transformer model for sequence-to-sequence learning. It was proposed by Facebook AI and combines ideas\n",
        "from BERT and GPT.'''\n",
        "print(summarizer(text, max_length=40, min_length=10, do_sample=False))\n",
        "```"
      ],
      "metadata": {
        "id": "eGMtQlURf9oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ T5: ‚ÄúText-to-Text Transfer Transformer‚Äù\n",
        "T5 converts every task into a text-to-text format, like:\n",
        "\n",
        "* **Sentiment**:  \n",
        "\"sst2 sentence: *I love it*\" ‚Üí \"*positive*\"\n",
        "\n",
        "* **Translation**:   \n",
        "\"translate English to German: *The book is on the table*\" ‚Üí \"*Das Buch liegt auf dem Tisch*\""
      ],
      "metadata": {
        "id": "DDvSmjq4gLPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "t5 = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
        "print(t5('''summarize: T5 is a text-to-text transformer model developed by Google. It can perform multiple NLP tasks using the\n",
        "same architecture.'''))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "dT5Py-2Pgrs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ GPT: Autoregressive Generator\n",
        "GPT predicts the next word given a left-to-right context.\n",
        "\n",
        "Useful for:\n",
        "\n",
        "* Chatbots (ChatGPT)\n",
        "\n",
        "* Story generation\n",
        "\n",
        "* Code generation (Codex)\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "print(generator(\"Once upon a time, a dragon\", max_length=30, num_return_sequences=1))\n",
        "```"
      ],
      "metadata": {
        "id": "5crwkATEhQgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öñÔ∏è Model Comparison Table\n",
        "| Feature | BERT | T5 | BART | GPT |\n",
        "|---------|------|----|----- |----|\n",
        "| Architecture | Encoder | Encoder-Decoder | Encoder-Decoder | Decoder |\n",
        "| Pretraining | MLM + NSP | Text-to-text | Denoising | Autoregressive |\n",
        "| Use for Generation? | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |\n",
        "| Key Strength | Understanding | Multi-tasking | Summarizing | Generation |"
      ],
      "metadata": {
        "id": "NVKr0I66iLSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Summary\n",
        "* T5: One model, many tasks (text-in ‚Üí text-out)\n",
        "\n",
        "* BART: Great for text correction and summarization\n",
        "\n",
        "* GPT: Best for fluent, long-form text generation\n",
        "\n",
        "They are foundation models powering today‚Äôs NLP tools:\n",
        "\n",
        "* ChatGPT (GPT-based)\n",
        "\n",
        "* Google Search (T5 variants)\n",
        "\n",
        "* Facebook AI (BART)"
      ],
      "metadata": {
        "id": "LrejXvUEjjSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Exercises\n",
        "1. Try summarization, translation, question-answering with **T5** and **BART**.\n",
        "\n",
        "2. Use **GPT-2** to generate multiple endings to a story prompt.\n",
        "\n",
        "3. Write a paraphraser with **BART** using a text2text pipeline.\n",
        "\n",
        "4. Compare the output quality of **T5-small** vs **T5-base**."
      ],
      "metadata": {
        "id": "GweoWj8Yjvfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚û°Ô∏è In Week 17, we will move on to **Retrieval-Augmented Generation (RAG)** and understand why it's important"
      ],
      "metadata": {
        "id": "nnkRJLWmj6LM"
      }
    }
  ]
}