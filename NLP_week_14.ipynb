{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNniIswdNbVJpODc486+iJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debojit11/ml_nlp_dl_transformers/blob/main/NLP_week_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Week 14 – Word Embeddings (Word2Vec, GloVe, etc.)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0KT6DLa3JNso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Motivation: Why Do We Need Word Embeddings?\n",
        "\n",
        "In previous weeks, we used **TF-IDF** or **CountVectorizer**, which treat each word as an independent token.\n",
        "\n",
        "But:\n",
        "- They ignore **word meanings** and **context**\n",
        "- The vectors are **sparse** and **high-dimensional**\n",
        "- \"king\" and \"queen\" are just as unrelated as \"king\" and \"banana\"\n",
        "\n",
        "We need a better way to represent words ✨\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 What are Word Embeddings?\n",
        "\n",
        "Word embeddings map each word to a **dense vector** in a continuous space where:\n",
        "- Similar words are close in distance\n",
        "- Relationships between words can be captured (e.g., analogies)\n",
        "\n",
        "Example:\n",
        "$$\n",
        "\\text{vec}(\\text{king}) - \\text{vec}(\\text{man}) + \\text{vec}(\\text{woman}) \\approx \\text{vec}(\\text{queen})\n",
        "$$  \n",
        "\n",
        "\n",
        "This is NOT possible with TF-IDF.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ghw548ewJZ8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧰 Word2Vec: Predictive Embeddings (Mikolov et al., 2013)\n",
        "\n",
        "Two main architectures:\n",
        "- **CBOW (Continuous Bag of Words)**: Predicts current word based on context\n",
        "- **Skip-Gram**: Predicts surrounding words given the current word\n",
        "\n",
        "### How It Works:\n",
        "- Feed in one-hot encodings\n",
        "- Use a hidden layer as the **embedding matrix**\n",
        "- Train using a shallow neural network to maximize prediction probability\n",
        "\n",
        "This leads to **semantic structure** in the learned vectors.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 GloVe: Global Vectors for Word Representation (Stanford, 2014)\n",
        "\n",
        "GloVe uses:\n",
        "- Global word-word **co-occurrence statistics** from a corpus\n",
        "- Trains a model where **ratios of co-occurrence probabilities** reflect semantic relationships\n",
        "\n",
        "Key insight:\n",
        "$\n",
        "\\text{word vector dot product} \\approx \\log(\\text{number of co-occurrences})\n",
        "$\n",
        "\n",
        "It combines the global matrix factorization of LSA with local context-based learning like Word2Vec.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8j5GKh84Jplr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚖️ Word2Vec vs GloVe\n",
        "\n",
        "| Feature         | Word2Vec     | GloVe          |\n",
        "|----------------|--------------|----------------|\n",
        "| Training       | Predictive   | Count-based    |\n",
        "| Context Type   | Local window | Global matrix  |\n",
        "| Developed By   | Google       | Stanford       |\n",
        "| Pros           | Fast, scalable | Captures global patterns |\n",
        "| Output         | Dense vectors| Dense vectors  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Cosine Similarity\n",
        "\n",
        "We often compare word embeddings using **cosine similarity**:\n",
        "$$\n",
        "\\text{cos}(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|}\n",
        "$$\n",
        "\n",
        "This measures the **angle** between vectors. Closer → more similar.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H2Eb2cQ-KGsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Example: Using Pretrained GloVe Embeddings\n",
        "\n",
        "We'll use a small GloVe model (100-dimensional vectors).\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load GloVe vectors\n",
        "embeddings = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = vector\n",
        "\n",
        "# Compare words\n",
        "def similarity(w1, w2):\n",
        "    v1 = embeddings.get(w1)\n",
        "    v2 = embeddings.get(w2)\n",
        "    if v1 is None or v2 is None:\n",
        "        return None\n",
        "    return cosine_similarity([v1], [v2])[0][0]\n",
        "\n",
        "print(\"Similarity (king, queen):\", similarity(\"king\", \"queen\"))\n",
        "print(\"Similarity (man, woman):\", similarity(\"man\", \"woman\"))\n",
        "print(\"Similarity (apple, banana):\", similarity(\"apple\", \"banana\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "23NUQQazKmB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWgSOMxRKpzh",
        "outputId": "0b4ca302-017a-4b90-ad23-5a82ac9c2fa9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 11:03:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-04-22 11:03:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-04-22 11:03:35--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2025-04-22 11:06:14 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43M1GRPMKxaZ",
        "outputId": "8059d21f-8b96-492e-94c9-35d5c77e2e85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "jTGaWfx5LdTX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe vectors\n",
        "embeddings = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = vector"
      ],
      "metadata": {
        "id": "1fobTzkjLevF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare words\n",
        "def similarity(w1, w2):\n",
        "    v1 = embeddings.get(w1)\n",
        "    v2 = embeddings.get(w2)\n",
        "    if v1 is None or v2 is None:\n",
        "        return None\n",
        "    return cosine_similarity([v1], [v2])[0][0]"
      ],
      "metadata": {
        "id": "0Pr_EImpLi8G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Similarity (king, queen):\", similarity(\"king\", \"queen\"))\n",
        "print(\"Similarity (man, woman):\", similarity(\"man\", \"woman\"))\n",
        "print(\"Similarity (apple, banana):\", similarity(\"apple\", \"banana\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8NjFau8LmG2",
        "outputId": "f201c48f-04ab-45e3-bc2f-2161d3066ac8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity (king, queen): 0.7507691\n",
            "Similarity (man, woman): 0.8323495\n",
            "Similarity (apple, banana): 0.505447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Real-World Use Cases of Word Embeddings in NLP\n",
        "\n",
        "| Task                   | Usage                                 |\n",
        "|------------------------|----------------------------------------|\n",
        "| Text Classification    | Embed each word → average → classify   |\n",
        "| Similarity Search      | Find nearest neighbors in embedding space |\n",
        "| Clustering/Topic Models| Use dense vectors instead of TF-IDF    |\n",
        "| Pretrained Layers      | Initialize neural nets with embeddings |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔚 Summary\n",
        "\n",
        "Word embeddings revolutionized NLP by:\n",
        "- Bringing meaning to vectors\n",
        "- Enabling semantic reasoning\n",
        "- Powering neural networks with better inputs\n",
        "\n",
        "> Next week, we’ll  then move Transformers, and BERT 🔥"
      ],
      "metadata": {
        "id": "SslnKZj4KpeA"
      }
    }
  ]
}